So I am going to do a quick summary of all the goody two-shoes stuff I do, so there is RailsBridge, like Josh said, this holds workshops around the world, nay are free to everyone, for women and marginalised groups.  If you want one near you talk to me.  Bridge Foundry, to get your technology into the hands of groups.  And tomorrow night GitHub is holding a patchwork workshop.  There is a link to all these things and a handy bundle I made for this talk.  If you want to RSVP, you can check it out there.  It also contains links that are relative to my talk.  That was
a joke.  That's fine.  Thanks, y'all, thanks.  I want to say a few words about code complexity.  So code complexity is a problem that everyone that's running a Rails app is going to run into
eventually if you don't have it.  It is easy to get a Rails app started but as it grows parts become
entangled with other parts.  We need to think about it more and more and need to start addressing every feature as a way that we can extend the future.  Secondly, high-level architecture decisions, it is the bad word, someone is afraid of some word.  So high-level architecture decisions, so whether to extract code or keep it in one application can have a huge impact on code complexity and where the complexity lives.  We should think about that as we think about architecture.  We should aim for something to reduce the complexity.  We are lucky we live in the age of Sandy Matz. Sandy has addressed how to deal with code complexity and she has done it well.  So I am going to try to summarise her points, a lifetime of work in one slide.  The gold standard to
remove complexity is that you should refactor to have each class representing a single responsibility.  That class could still have complexity inside of it but the class itself should
conceptually be simple.  Be shorter, simple and better names and make it easier to change in the
future.  But those sorts of refactors can be a huge amount of effort and you don't necessarily have a lot to show for it when you are done and you haven't shipped a new feature.  So because of that it is necessary to compromise.  You want to find places that are tightly coupled where your application, where you pass through that code often.  If you have complexity in a place you never use, you don't need to worry about it that much and that is fine.  So stable code paths that are not changing or at the end of the line, the complexity won't get in your way.  Sandy, everyone.  Her link and her book and blog are in the bundle.  One thing she hasn't jumped into are the conversations around service-oriented architecture.  I loved Sabrina's talk earlier how to rewrite a complex Rails app into services.  That was great.  I am going to give a contrasting perspective of how I messed it up.  We will compliment each other.  It is useful to bring the two arguments into one conversation because I think service-oriented architecture has been misunderstood as an approach to managing complexity and I don't think that is what it is.  If service-oriented architecture is new, and I am sure at this point in the conference it is not, it is SOA for short, a way of building small services that represent a distinct function.  The goal behind SOA it is
easier to maintain, more sustainable to scale than supposedly a large monolithic application.  So there are a lot of talks and blog posts.  There are some books and they are focussed on the
benefits.  Lauren, yesterday, gave a really great talk about how to extract a service from an
existing application.  Sabrina did the same thing today.  I would like to see more blog posts and talks and things like that that investigate the ultimate success or failure of the projects.  I think
we are in the beginning days so maybe it is not available yet.  I am going to start trying to do that
in this talk and I look forward to more evidence as it comes in.  I would like to see what the success or failure ultimately is of breaking the large applications into smaller services because I think SOA has, it works in some specific cases but maybe we shouldn't be trying to replace all of our large Rails apps with services.  There some real downsides.  It is possible to end up with services that are difficult to test, confusing to debug, five times the work to deploy and to echo a point made by Coraline yesterday, a bad SOA implementation can create an eco system of services that reflect your bad app with the added complexity of network failures.  It is not necessarily a win.  What happened? Is that okay? Okay.  So everyone is awake.  I'll just let you read that.  It's cute.  This is me in real life.  So I brought three case studies today of failed SOA implementations from my own experience.  I see them as guiding me now to see what the right and wrong cases are.  The stories are true but I changed the domain model to protect the guilty. Specifically, the code samples are now the adventures of hatsforspacedogs.com.  So the first case study, extracting tangled code.  To set the stage, the team got a request for an old neglected feature on the site that would let space dogs vote on hats and then we would sell the winning hats on the site.  Thanks for just going with this... so most of the app was reasonably well factored but we rarely touched as part of the app, it wasn't core to our functionality so it became more
tangled.  The entire feature of this was one long function where it mixed behaviour with presentation and weird bug fixes over and over and over. It is a really simple feature and so it is

amazing that it became dauntingly complex.  I think it is because it had no structure.  The server side code for the feature suffered a different problem.  Hats you could vote on were the same classes as the hats you could buy even though the behaviour of the two were entirely different. To make it more confusing, if the hat ended up winning the contest and we were going to sell it on the site, we created a second hat record and they had an association with each other.  So is anyone, like, angry yet? That's a good response.  So it is a confusing data model we are working with.  If we had been trying to manage or complexity here we would advice of Sandy and refactored it into two separate classes that's is now wrapped up in the same class.  We should have stuck to the single responsibility principle but we didn't.  We wanted to solve some problems with the existing code.  In Rails it violated the principle, it violated behaviour for buyable hats and votable hats.  Secondly, the front-end code was so confusing it seemed nearly
impossible to refactor.  So that's all to say we had unmanaged complexity.  We thought we could see clear lines around what could potentially be extracted.  We wanted to have an app of votable hats and votes.  We knew something about SOA, we knew it meant creating small, well defined, easy-to-maintain services.  It sounded great.  So we started rewriting those features and new application.  We ran Rails new and defined the data model we had always wanted and then started to plan how to import the old votable hats records into the new hat records into our new votable hats application.  This was glorious.  It was fast moving and we were doing the thing we thought God wanted us to do.  Then we started working on a migration script that imported votable hats and its association into our new application.  We had to fudge it a bit to fit our new data model.  That's where things went downhill really fast.  To give you a sense of the
complexity we were encountering, did the votable hat win the contest? If so is it available for purchase and should we link to it? If it is available for purchase but slow in stock should we put a
banner on it.  Or if it won the contest but not yet available, the link to preorder rather than
purchase.  So we - we didn't give up.  We started with our main application and the new votable hats app.  We connected to the old data base to get information about the existing hats, it should have been a warning sign.  Then we realised the main app needed information about votable hats and we couldn't wait for it to be done.  We should pause here.  This is the diagram of doom.  If you find yourself drawing this diagram you should stop, go and get a coffee and maybe get a new job, you should stop.  So when it tells you you have drawn the lines around your new service incorrectly, we were not actually extracting an independent service here.  So at this point we would be on firm ground if we called this a disaster.  But let's keep going because we kept going. So we realised there is so much complexity in the hat model and its associations the best thing would be to reuse all of that in the new application.  So we packaged up all our models into a
gem which we started sharing with our new app.  Cool... so just to say a few words about this awesome diagram of doom, rather than separate out the votable app behaviour from the buyable
hat behaviour we created a system that mirrored the complexity of the original data base.  Not a
win.  Let's reflect.  What can we learn? First and foremost we did not draw the correct lines around the new service.  We do it around a customer facing feature rather than a logical feature in the app.  There were a lot of things going on in the future.  We drew the lines without understanding the code almost to avoid understanding the code.  We did it because we thought SOA would be a way to manage the complexity.  The hat model was a problem for us and instead of extracting some unit we understood, we just started trying to extract before we understood.  We realised it was a bad idea and backed it out and it never went into production. Ta-dah!! GitHub.com is a giant monolithic written in Rails.  There are portions not tested and it makes it risky.  To help with rep factors, it maintains a project called scientist.  I link to it in the bundles.  It works by letting you define an alternate code pact and it will run the old code path and the new code path and reports back if they agree or disagree.  The user will get the result of the old code path, a way to dark ship the rewrite and find out how it is doing and all the mini situations.  Sometimes it will reveal bugs.  Jessie Toth who wrote a lot of the permissions in GitHub gave a talk how she used scientist to write the code.  That was gnarly, poorly understood code.  When she rewrote it and it shipped, it was rewritten to have tests to be easily understood and be ostensible.  It was impressive and took as long as you would guess.  Shout out.  On to the
next case study.  Back at hatsforspacedogs.com.  We set out to build an identity service.  We had complexity in two models, the hat model and the space dog model, our version of the user model.
We thought an identity service would help us manage complexity by isolating the complexity of space dog model.  So setting out to bid this identity service we thought we would solve a few
problems.  We went to isolate complexity and it was a small improvement because we defined our first service for the votable hats based on a customer interface feature.  Now we were
defining it around a Rails model.  Secondly, we knew the code in there was confusing and complex and we thought one thing that would help is have team ownership over the model so we
thought extracting it into a separate repository would help with that.  So we knew that we didn't

want to end up with closely couples services like votable apps but we saw it as way to get small services.  This time we thought it would help define team ownership and so it happened.  So, first, we didn't give the identity servers a data base of its own.  This was a grand idea.  When requests would come into the main app, it would hit the API for the identity service.  It was kudos but it wasn't terrible.  Then we built a mobile app.  You might think it would be a sane thing to think that it would head the identity service and the database and get back a response. But we didn't sit down and understand all the places the space dog model was doing things all over the application.  We were never able to fully extract the identity service.  So every time the identity service needed to call back to the main application, so two things were hitting the database every time before you get a response.  This is a disaster.  This is actually a much more dangerous disaster in my opinion that the votable hat app and not just because it went into
production.  It also brought with it a lot more operational complexity.  The identity service wasn't capable of functioning without the main application and the main application wasn't able to function without the identity service.  It meant they both needed to be up all the time.  If either went down you couldn't shop for your hats as a space dog.  We had shifted workload onto the operations team without a clear win at all.  It became the worst of all words.  We had added a single point of failure, one that wasn't widely understood and we didn't relieve the main app of any load.  This is where I would like to talk about SOA and scaling.  We naively thought it would improve scalability.  Just numbers that are made up, however many people are at Rails Girls and RailsBridge are made up, say we needed 2,000 serves for the main application and we added 200 it would be an improvement.  But they had the same-time requirements because users need to believe able to sign in.  Here we can use a cuing theory to see how it would hurt scalability.  So if we entirely ignore the extra network calls, if have 2,000 servers serving the request for the main application, they still have to get in line again to authenticate.  They need a second step and there are only 100 servers for that.  It is creating an artificial bottleneck we didn't need.  Instead, if it
can be served from one large pool of servers then a certain set of requests will get there faster. Here is the version where you don't have the bottleneck.  SOA is a bad strategy in these cases
because maintaining the speed of the user experience means we would scale up, so neither would
become a bottleneck.  The calculation changes if you have different up time requirements.  I know this is a kind of a weird thing to talk about at a Ruby conference so I included links in the bundle.  The other main goal we had with the project was to define team responsibilities.  There is an alternative that I pulled from GitHub.  We have a inglorious and super simple way to deal with it.  We have a file and for every file in the application, there is a team assigned to take care of that file.  It saves us a lot of work.  Everyone knows who is responsible for a file.  If I want to change something then what is not my file to change I need to talk to them.  Sometimes simple is okay.  This brings me to my last case study.  This is one that I don't think had to be a disaster the way the other two were just doomed, but the implementation made it one.  We had a monolithic Rails application.  The really great things about monoliths is you need one set.  But mobile webs and mobile apps and apps that were backing the web experience.  We wanted to share them instead of having to repeat them.  These are things to give you a sense of the shared assets, things like headers, which are visible everyone, log in models, some space dogs have really big heads. Thanks for giggling.  So as you are thinking about what we should build here, one main thing we wanted to solve, we only saw one problem, having to rebuild the same assets in every service. Here is what we came up with.  Various applications that needed the assets and we decided to
add a gem server.  We love gems.  So each app could include the gem and get all the assets in one swoop.  It is easy to package and easy to include and you can reference it easily within your code.  The trouble came when it was time to update the assets, when the assets changed, you want to change them every where but it is not how gems work.  So in the case of a site redesign, we could increment the gem version but implement it across all the versions and deploy it at the same time.  It was common when we were trying this we were shown inconsistent experience while trying to deploy everything.  It is not a great user experience but it is still an improvement. It drew the right boundaries around the service.  But the implementation became a pain point because we forgot an important business requirement that we would need to change the assets uniformly all at once.  That becomes a problem of cash invalidation.  So if we were to add that requirement we could have in retrospect improved implementation.  We didn't do it.  But it is now a better idea.  Is quarterback in here? Okay the people from America are nodding but you don't get to nod for that.  It is fine.  Instead of cashing the assets in each application we created an assets service and the application with request the service directly instead of cashing them inside the app.  It would give us other problems, we needed to carefully opt.  The CDM.  I think it would have been worth it for us.  So at GitHub we have the problem too but hide it well.  We have a few different packages to share assets across apps but crucially the most parts of the
customer interface application are all one Rails app.  It takes a while to update in some places but

the majority the customer sees is in one place.  So in conclusion, the stories of SOA gone wrong help to point some really good cases for SOA.  The first and most important to remember is service-oriented architecture is not a mechanism for managing complexity.  If anything, SOA is a possible outcome of refactoring your code complexity, but it is not a strategy to get there. Separate repositories do not magically create ownership, that is an organisational problem. Secondly, the overhead of service-oriented architecture is justified if you need to run in multiple places.  In the case of assets, we wanted it to run in different apps.  Lastly, service-oriented architecture, if it is part of the system with different up-time requirements can be a useful thing and let you degrade gracefully.  But it is essential when you are building it out you don't want to create a bottleneck.  That can be harder to measure.  This is the most unpopular thing, you need
to invest in refactoring so it is something you can work with and no architecture decision will get you out of that.  I would like to thank Sam Brown, who created exporting.com.au where I got the
delightful illustrations and my friend Kelly who told me about it.  I drew skirts on the key notes
so they were like me and then I did it by 53, it is where you can get all the links, thanks.
