So, I would like to take you back to the dawn of the age of computer storage.  I mean this quite literally.  When programmers think about bits, we typically represent them symbolically as a series of zeros and ones.  Tom Stuart had a slide similar to this in his talk yesterday.  But bits are not actually zeros and ones.  They are in fact electronic components, real physical things that you can touch.  In the beginning a mere 67 years ago, there was only one of them and it looked like this.  This is a replica of the world's first

transistor.  If you have done any work or play with electronics this may be a more familiar picture of a transistor.  It has three terminals, that is characteristic of a transistor.  This is the symbolic representation of a transistor.  One terminal is called the collector, or the drain.  One is called the base, or the source.  And the last is called the omitter, or the gate.  I like that each one has two different names to make it really confusing.  So the way this works is if you take a power source, let's say a battery, and connect the negative terminal to the base and the positive terminal to the collector, nothing happens.  That's basically the equivalent of a zero.  But as soon as you touch the collector to the omitter, electrons will start flowing through to the base of the omitter and from the collector to the omitter with a higher amperage.  Quick show of hands: How many people understood how a transistor worked before I just explained it? Okay.  Hands down.  How many people still don't understand what it is after I've explained it.  A few of you.  Thank you for your honesty.  So the good news is doesn't matter.  You don't need to understand how a transistor works to use or program a computer.  The thing that makes this possible is something called abstraction.  What is abstraction? I'd like to share a quote from John Loch concerning human understanding.  He said there is three main things the mind is capable of.  1.  Combining two or more simple ideas to create a complex idea.  2.  He said you can take two ideas and not necessarily combine them but just sort of put them next to each other and compare and contrast. The product of this exercise is understanding the relationship between these two ideas.  3.  The third main thing the mind is capable of is abstraction.  Basically, you can take a concrete idea
and generalise it.  So that's Loch's definition, this is mine.  So in the same way we can all use and program computers without necessarily understanding how the fundamental pieces work, how each layer below the level programming app works, we are able to do it it anyway.  When soft programmers think about a bit, we don't think about something that looks like thick.  Maybe something like this.  You don't need to understand how a transistors work to understand it works as a switch and we can abstract that to symbolic of zero for off and one for on.  By giving hardware bits a symbolic notation of zero and one it allows us to start using bits as counters. Unfortunately we can only count up to one, which is not useful.  If we need to count higher than one we need to give our bits some structure.  This brings us to the byte.  They free us from thinking about individuals ones and zeros.  Instead we can think about numbers to 255.  Once we have done that it is a short step to letters, numbers and punctuation.  Abstraction allows us to work with things without understanding them and reason on problems.  We can think about
things in terms that are understandable to us, like letters.  We can look at the letters and get some human feeling about what they mean, as opposed to the bits which are only meaningful to the computer.  We used to program computers like this, telling which individual switches it should turn on or off with a punch card.  In some Third World countries, such as Florida, voting is still done this way.  This is some S86 assembly card that I borrowed from Tom's talk yesterday.  It is higher level than writing zeros and ones but it is still tied to a specific computer architecture.  It registers and specific addresses.  Instead of referring specifically to memory, why not give that memory address that makes sense to a person.  Languages with variables tend to be more abstract to languages without variables and I think this is an improvement.  Almost all programming languages have variables.  It is better than this.  Strings are an abstraction over characters.  Not
all programming languages have strings, some of them just have character arrays.  The array sizes can only be constants.  This mean we run the risk of a buffer overflow since we have to guess in advance how many characters we need.  There is that null terminator that goes at the end and a whole category above that can be caused by forgetting to add one or by trying to add things to the character array after the null terminator.  That won't work.  Strings are an abstraction that allow you to operate on a higher level.  I think they are quite a nice one.  Does anyone know
what the C program output is? A hint? That number, I, stored in the int I is int max.  When we add one to it, any guesses? You will get negative 2 billion 147 million.  That's not very user
friendly, right.  In Ruby we don't have this problem.  We have fixed numbers and when we
overflow, we get big nums and we don't really need to understand that distinction.  In my view, Ruby could hide it away and just have num or integer.  Not really important to you, you don't need to know the difference.  As a result you can have infinitely long numbers in Ruby and it is constrained by the length.  So functions are abstractions as well.  They are abstractions over behaviour and languages that allow you to define functions are more abstract than languages without functions.  It is not such a controversial statement.  I think functions are quite popular in programming languages, so that's good.  But blocks or not? Not every one language has blocks or closures.  This is a high-level feature that is quite nice.  Again, I would argue that the thing that blocks allow you to do is to define behaviour on the fly.  If you look most Ruby DSLs, the way they work is heavily tied into using blocks, something like RSCVAC.  It leverages this feature in Ruby so you can operate in a higher level, a test domain, for example.  Or Sinatra allows you to operate in the HDDP domain using blocks.  So again, I would argue that languages that require

you to manage memory manually are less abstract.  Languages that perform garbage collection are more abstract.  This is terrible, basically.  If you try to, if you call that freed twice instead of once later on, then you will get an error and if you try to use pointer that is freed, you'll get an error.  There is a whole class of errors you can solve by getting rid of manual garbage collection. Grace, who worked on Cobalt, believed it should be written in a language close to English.  I want to take you through the history of programming languages.  Just so you can see the trajectory.  I think there are trends and lessons to be learned from this.  This chart is basically just one very dynamic slide, but don't worry too much about the Y axis.  It is not labelled.  Just think about it as the values of being relative.  So it one programming language is above another, that means it is more abstract.  We won't say how much more because these things are hard to
quantify but I think you can make a clear case in the ranking, basically, forget about the magnitude.  At the most basic level, the first computers, we had machine language.  Shortly after
that we developed assembly language, again, specific to a particular hardware architecture,
talking to addresses.  A few years later, I believe it was 1958, we got lisp.  It was much higher level.  Don't worry too much about the magnitude but it was one or two more orders of language more abstract than assembly line, which itself was more abstract than machine language.  I
would call it progress.  The year after lisp was released, we got Cobalt, Grace Hopper's language. In 1960 we got Alpo, which is similar to Cobalt and Ruby, certain Ruby syntax can trace its roots
back.  They are interesting because they are the first two languages that tried to define a language standard and say the language itself conforms to a particular standard that people can write to,
which I would argue is a form of abstraction because by locking down the standard you can use
the language across different hardware architectures, because at the time hardware was widely varied.  If you could standardise it was a nice feature.  Standardisation was nice feature.  Then a
few years after that in the mid60s, Simular was released.  I think you can trace some of Ruby's
roots back to it.  It never really got very popular but it had many of the sort of object oriented features that inspired features to some.  Then came Fee in 1970 - C, I would argue is a very low level language.  One level below assembly.  It is standardised and abstracted across several hardwares, portable as long as there is a C compiler.  But I would say it was a bit of a step backward in terms of abstraction.  That said, compared to all the other languages, C was really popular.  It continues to be popular relative to these other languages.  It is arguably the most popular language today.  I think the reason why C became so popular was because it gave you a certain amount of control that you really needed back then.  These abstractions actually have a cost.  The cost is that if you are writing code, optimising the language for a human instead of for
a computer, it is not going to be as efficient.  C, by writing code that is optimised for a computer, allows you to get performance in an age, really good performance for really low-level systems in
an age when hardware was very, very sort of slow and constrained.  Memory was constrained.  C
became popular in this environment and continues to be popular today.  The other thing about this time, there wasn't that much code.  So almost all code by definition was low-level code.
People were writing the first operating systems, the first servers.  People weren't building web
applications because the web didn't exist.  People weren't building games.  Having C and having the control over the language and having a little bit more abstraction than assembly language but having some control, C hit the sweet spot for non70 and began to take off.  Then a few years later, small talk came around.  Small talk took some of the concepts from Simular and built on that.  I think it is a very significant language and at the time, again, continued this trajectory after the step backwards to C, more abstract languages over time.  So then in 1983, the year I was born, C plus plus came out.  It took some of the high level concepts, the object oriented
principles and brought them to a C-like language.  But it did it in way trying to be compatible with C so it gave objects and added new objects to C, which was C plus plus quite literally, but not necessarily abstracting away those low level details.  So you still have the same control as C but with the new idea of objects.  Objective C came out around the same time and I would say another attempt at making a better C.  Then I believe it was 1991, python came out.  Python was really a big leap forward towards more abs tract programming languages.  It was probably more object oriented features.  It it was a wholly new thing.  You could run it on the fly.  It was true of some earlier languages as well.  Perl also came out around then, but at the time perl was first released it wasn't so I would rank that below small talk on this scale.  Then 1995 was a crazy year, probably the most significant year of programming language release, it was a very significant year.  It was the year that Ruby was released and JavaScript was released and also Java was finally released.  I sort of slotted these in.  Ruby is up there with python, I put it above python because I'm biased.  Java is cool because it runs the sort of innovation with Java, it
doesn't out put machine code, there is a virtual machine.  The way it was able to gain portability was not by having a compiler for every system but a virtual code.  I think it was made possibility in the mid90s when you can run a machine on top of hardware.  And Java also created a platform

for the virtual machine.  Scholar came out trying to be a better Java.  It looked at the mistakes Java made in its design.  I would say it is a higher level language than Java and probably better in most ways.  Closure is another language I think better than lisp.  It is a full-featured lisp and
again hosted on the Java machine, which is quite a nice platform.  Another trend upwards, so go, this is getting into relatively modern history.  I think it was released in 2009 or something like
that.  Go is a lower level language than almost all the ones on the list but it is not trying to be a
high level language.  Go is trying to be in their manifesto a better C plus plus, people were still trying to write that in 2009 and they thought people should have something better.  You can look at it as a low level language but when compared to what it was it intended to replace, it was higher, garbage collection was built in.  There is other ways in which go is lower level, right.  It doesn't have a lot of the features that C plus plus has.  But overall it is a higher language.  And then rust I would put in beneath go.  Because it doesn't have garbage collector, for example, but again, it is a lower level language when compared to Ruby and python but higher than C.  We see the trajectory getting higher and higher level.  There is one more.  Swift again follows the same pattern.  It is not higher level than Ruby, it is trying to be a better objective C.  Over time we are moving higher and higher and higher up this abstraction ladder, basically, but nobody is trying to make a better Ruby.  People try to make a better C.  People have tried to make a better C plus
plus and go. People have tried to make a better objective C and swift but nobody is really attempting, or least they haven't got so popular or much traction to make things that is more abstract than Ruby.  So the rest of the talk will be focussed on that.  It is still the highest level
programming language in the world, according to me.  But I think there is a good argument to be
made for that.  Maybe not the highest level in the world but take into account wider use it is a high level language but it is over 20 years old now.  It was released in 1994.  If Ruby was being
reinvented today, if somebody was creating a new programming language that sort of had the
same design goals as Ruby, a programming language that just does things for you, these are sort of the things I would do to make a higher level language in 2015.  I would kill symbol.  So symbols are basically strings but they never go away.  So the idea is that you can have a symbol and if you compare it to another symbol it is not allocating, when you create or reference the symbol again, it is not allocating new memory, it is pulling it out of the symbols table.  So it is a low level thing, a performance optimisation.  Every time you need a symbol you don't need to allocate your memory like a string, you simply have a reference to it.  But in Ruby 2.1, there is a new feature where you can free strings, this is the description of it, that's the issue number on the Ruby bug tracker.  You can read the details but if you say literal.freeze, it will behave have like a symbol.  These are referencing the same object, which was the whole point of symbols in the first place, what symbols were trying to solve.  Since Ruby 2.1 it has been solved in strings.  Strings are moving closer to symbols in this way.  You can symbolise a string by calling to sim or call a freeze and it has this property for you do that.  Then Ruby 2.2 came out a short while ago with this, symbol garbage collection.  So the remaining difference is that strings would be garbage collected and symbols wouldn't.  But it caused a problem where a user would have a very long hash and Rails would turn it into a symbol and it was a big vulnerability.  So Ruby said it is too risky and we will deal with it.  Now they are becoming more like strings, so it two constructs are converging.  I think it is an optimisation that might have made sense in the mid90s when being developed but it doesn't make sense anymore.  To take you back 20 years to remind you what it was like back then, in the mid90s when Ruby was being developed laptops had 8 megabytes of ram.  Today, this has 8 gigabytes of ram, it is 1,000 times as much.  We don't need to be concerned by string like things taking up memory.  We have plenty to spare and it is cheap.  We are garbage collecting it so it is fine and great.  Next on my sort of chopping block for Ruby, I would kill floats.  It is to say I wouldn't get rid of them entirely but I would have something
better, I would have a better decimal class.  Float is not a general purpose construct.  Maybe it made sense in the '80s when it was developed but we have the memory and CPU cycles to do that.  There is all sorts of categories and mistakes that happen because you use floats that wouldn't happen if we had a more precise decimal.  This is the next one.  The first tweet was from January and this is a couple of weeks later.  Garry quit programming because of floats. Yeah, floats suck.  Why do they suck for those not familiar with had suckitude of float? It seems reasonable but it is totally false.  Why is it false? You would think computers by this day and age would be smart enough to figure it out.  People get into trouble by storing money in float and losing money that way.  So basically these rounding errors can occur.  There is no reason for it. You could just have money class with two digits of precision and maybe something like a currency attribute.  That is nice but it is not built into the language.  Float is.  It gives people incentives to do the wrong thing.  Every program language has problems.  Ruby implemented a standard which was developed in 1985.  To take you back to 1985, I took you back to the
mid90s, now the mid80s, the state-of-the-art computer was a MacIntosh 512 K.  They put it in

the name because they were bragging about how much ram it had, it had four times as much as the 1984 MacIntosh.  It also had a 7.8338 meg processor.  They needed a float to store the precision of the processor.  Ironic.  So in 2015, we are like rounding off hundreds of megs now, let alone fractions of a meg.  I think we have infinite precision integers.  Why can't we have infinite precision decimals.  Next on the chopping block, nil.  So how many of you will have
seen this before? This is Ruby's way of telling you.  You can wrap it in a container object and the programming language ensures you handle it correctly.  This is a feature in swift and modern
languages with better systems.  A shout-out to Tom and his talk yesterday, Ruby shouldn't have
optionals.  There are a lot of advantages to declare an array of a specific type and then sort of limit adding things on to the end to be of that specific type.  I just made up some syntax for my
proposed future programming language.  So this is how you would say an array of integers and
an array of letters or strings and it wouldn't let you, the programming language wouldn't let you append to that array something that is not of the correct type.  This not only saves you from certain errors because it is typical, it is odd to have mixed type arrays.  I would argue it should be default and have something like a tupple for a mixed type.  The other nice thing about this you can get a lot of performance optimisations.  It is higher level but lets you optimise code at a
lower level.  The way you optimise if you know everything of the array is of the same time, when you want to index, if you know the array is an 32-bit integer, you multiply 32 times 1,000 and
just jump into the middle of the array.  I think the next version of JavaScript has it, I want Ruby to have it.  Methods overloading.  It is like a contriving way to calculate the area of a triangle.
There is simpler ways to do it.  But if you have two sides you can calculate the area of a triangle
this way.  I thought it would be cool if you could specify the degrees or the third argument of the degrees as an integer or radiance as a decimal.  Again I am using the syntax I just made up.  But
the nice thing is you can sort of dynamically dispatch to different methods that have the same
name as long as they of different types.  Elixir has it and lots of cool programming languages have the feature.  I would like Ruby to have it because it is quite nice.  If you wanted to do this, otherwise we would need ugly conditional code and you get to call the area method from within the method but it is not recursive.  I think it is cool.  I think methods, return methods.  Up until Ruby 2.1, methods returns were nil which were useless.  In 2.1 they starting returning their names as symbols which was slightly better.  It would be better if they returned the method they defined as a method object which you would pass to another method.  You could have a memo class method.  The method would return itself.  It looks cool.  You can define things like private and protected, or public I suppose.  I think those wouldn't need to be key words, you could take them out of key words and make them methods that act on return methods.  This is like a real bugaboo of mine.  There is too many ways to define it in Ruby.  You can do it with the desk key word and the proc key word, it is slightly different from the Lamdur and there is two different
block syntaxes and blocks behave like procs but they are not objects.  You have to understand the different rinses and the syntaxes and semantic differences.  It shouldn't be necessary.  We should
be able to unify these.  I don't know if we can get it down but get it down to two would be nice.  I
have a proposal for that.  It is borrowed from JavaScript and other languages.  The function in JavaScript is function, so it is confusing.  We should replay prop with def.  You say def and nothing and have the block body and return it a prop. It compares nicely with methods returning methods so it makes it consistent.  Better concurrency primitives.  I think it is the last one. Basically if you want to do concurrency in Ruby it looks like this.  You are dealing with threads and they are an operating system thing.  Let's say you have a collection with 1,000 items that will spawn 1,000 threads which your operating system will not deal with well.  The fact you have to know what a green thread is, it is bad.  All you should have to do is say take the code, itemise the process and parallelise it.  Maybe it will do something smart and dispatch it to a thread pool with a fixed number of threads.  I don't really care which one Ruby gets, as long as it is better than thread.  Other thing it has is fibre, but that's worse.  Because it is like not cooperative so you have to schedule the fibres yourself.  To be honest I am not sure, I am a bit afraid of fibres, I don't
think I would ever use them and I almost never see them used.  They are not that useful but we need to do concurrency.  It is hard, we shouldn't have to think about it and have nice objects that
abstract it away.  Just say parallelise it and it works.  That's the talk, thanks.
(APPLAUSE) I just want to say a big final thank you to all of the organisers who did an amazing job.  Rebekah on the keys, kicking arse.  Our great moderators, Josh and Keith.  Overall an
amazing conference.  My first time coming to Melbourne and Australia and it has been great.  So
thanks so much for having me.
