So today I would like to speak about DevOps and whether it is a fallacy, dream or I don't know, both, neither? And it is based on the story of how the company I worked for had handled thousands of writes and reads per second in a stack, all without the operations team achieving 99.7 without working hard.  So I thought share and help other people to build applications that are resilient that don't necessarily require an ops team.  It is fast.  The shopping mall of the future, if you are not familiar with it, think of it between Tumbler and Twitter, you
can follow stores and people and collections and it is personalised.  You will just get lost in it. We have got 350,000 stores represented, 20 million products and these have been saved over
3 billion times by users.  Think of us as Pinterest but pretty big.  We are pretty small team, half are engineers.  Next I want to talk about people, talk about stack.  What is a stack? They are
doing what? Stack is really a loaded question and that's why I put the slide together.  It is not

meant for you to read into it right now, maybe dig into the after you watch the presentation, but the point is today's applications, Ruby, Rail rely on such an enormous amount of components and tools as well as paid services.  So it is really kind of silly to be talking about stack unless you
have two hours to kill. There is a lot of stuff to talk about.  Next we will quickly - I want to talk about the traffic to your app.  So if you are building a Rails or Ruby application for mobile or
whatever, you are receiving traffic.  If you are building a website you can expect something
along the number of 100 requests per minute.  If it is something for the public and a more popular application, maybe 122,000 ARM.  Once you cross 1,000 RPMs, requests per minute, you are dealing with skill issue for sure.  The top four, Pinterest, Facebook and Twitter, it is a unique case.  Most of us do not have to solve the problems that these guys are solving.  You may have to solve the problems at 100 RPM or 50 or whatever.  So that progression from this really small application to kind of mid-size and big applications is the story I am going to tell.  So what does it talk about? I want to review ops, DevOps, cloud, how it is changing the landscape and point out key patterns that reduce stress, particularly with Ruby and Rails and generally bring up the question does a small startup, does it actually need an apps team, per se?  We will start with the basic, the DevOps, the first definition is from the Wikipedia.  It is a mouthful, terrible.  The second one is more interesting.  It is a quote from Forester report, "Today millions are confused about what DevOps means to them".  It is interesting, we took development and ops and merged them together and something happened but nobody knows what.  Another source says efficient times are deploying codes 50 times more quickly with fewer failures.  High performance.  These
are tangible things and we can look at that and say "that is interesting, what is going on here?" So it is important to kind of understand how we got there.  So traditional, heavy agile.  I call it
heavy, most companies call it agile.  It is a path where you have product pushing requirements, spending weeks manually testing it, into operations.  I have been in a company like that.  It was
difficult, it was terrible.  It is a bad way to build software.  It is hard, unnecessary process.  A lot of pain.  So I argue that you don't need those last two up until a certain size.  It also depends on
what application you are building.  Maybe for the financial markets, your QA team is a necessity but building a social network or trying something out, you don't need that.  You can be much
leaner.  So really quickly, let's talk about traditional ops.  Traditional operations are generally responsible and task with what op time stability, on call, fixing site at night, backups, disaster recovery, the sort of things ops people do.  They are responsible for hardware and networking
and colocation and data centres.  The cloud changed a lot.  The cloud allowed us to get rid of the things like hardware and now they are taking care of the cloud provider.  It is phenomenal.  It is a
key ingredient to make DevOps possible.  It is important to acknowledge.  Next, let's talk how to
build software in a simpler and lighter and friendly way.  I will start by kind of declaring the early company roles, based on our story, what we wanted to get to when we first started we had
about six engineers, three pairs.  We wanted to maximise the speed, we weren't sure what we
were building, we did but we wanted to experiment and launch things quickly and get them out there as fast as possible.  We called it AgroAgile in comparison to heavy agile.  We wanted to scale as we went, meaning if we climbed up, we wanted to be in a place where we can scale an application together with traffic. It meant we had to continually make smaller investments into performance, like adding the cash early on, adding infrastructure, so when suddenly traffic hits you, you can cash the two extra pages and suddenly it is better instead of, "My God, we don't know how to cash in at all and we built it in a way that prevents us doing that".  Investing in little things as you go along means you are thinking ahead.  We wanted to be able to break things and not be blamed or criticised about it and just learn.  I'll skip this but the final one is we wanted to remain in control of our infrastructure.  I tried Hiroku and various other cloud providers and, granted, the landscape has changed dramatically over the last three years but Hiroku was prohibitively expensive.  We wanted to be in control of our stack and environment.  While moving really fast, it is hard with ops.  We hired smart engineers who enjoyed ops work.  But they approached it as software engineering and approached it with the same rigor as
code-building applications.  Not having ops meant we had to deploy our op to the cloud ourselves and learn how to provision nodes and get balances up and running, search engines,
how to make them replicate and store addresses, all those things.  So to today, our application is
100 per cent hosted on joint cloud, it is a boutique cloud provider.  It is interesting, if I have time
I'll talk about it in the end.  We are 100 per cent automated with chef.  We went through a 10,000 per cent growth in traffic in six months, in fact we never went down.  We were slow occasionally and the users were, "My god, I'm going to shoot myself.  I can't browse my products", but we never went down.  So that was remarkable.  We did achieve 99.7 per cent op time, it wasn't hard but it wasn't the goal.  If it has to be done occasionally, that's life.  If you remember early Twitter, that doesn't seem to affect the success of the company in the end.  So the other thing is all engineers at the company only get 1 or 200 pages a week.  That is small.  All the juniors are on

pool rotation.  We still have no ops team but lots of ops work.  How do we do this? So the next part of the talk is more focussed on - that's my hope, that you can walk out of here with a very specific set of ideas and potentially patterns that you can apply directly to maybe your environment so you can reduce stress, reduce the amount of pages, reduce the amount of
whatever is plaguing your application.  So it is much more aimed at practical advice.  So the very first one is the automation and deployment.  Infrastructure is a first-class citizen and has to be treated this way.  When we develop user stories, when we build the features.  They develop infrastructure work with their features you want to chef and repo.  You added whatever the service needed to bring that up.  So all of that is happening in parallel and most of the team is learning how to use both.  That just creates a really good environment where lots of people understand how things are put together.  We also run it continuously in production, as opposed to another model where people use where they just don't run it, but run it by hand.  I find it dangerous because you never know when it was run last time.  If you continue to run it and something goes wrong, you know you only have to look at maybe the last 30 minutes to figure
out what happened and have time to fix it.  We don't do continuous development, we do incremental deployment.  We roll the code out everywhere and start watching the errors and see
what is happening with the servers, using logs and other things.  Usually a pair does that.  If
everything is okay, after two or three minutes they continue rolling the rest.  That is our deploy strategy.  It is barely manual.  It requires literally two commands.  Cap, deploy and roll.  I really like having that and not having continuous deployment, specifically as a leader of the company,
because I want my engineers to be closer to infrastructure.  If everything was just going out there
for every check and continuous deployment, and I think at some point when we grow large enough it will absolutely happen, but until then I think it is valuable to look what is happening
and try to understand how the code affects production.  The second one is probably the most
important part of this talk. It is how to build infrastructure.  The truth is I have been doing software for 25 years and I have seen the first dot-com boom including hash libraries, and today it is so much cheaper and easier than before.  It is literally a crime not to do it.  How to do it? If you remember one thing out of this talk, it is put HP proxy in front of everything, put it in front of your mum, your database, your elastic search.  It is amazing, stable, fast, it will save your life.
It will basically route across multiple servers on the back end, it will take it out, take your request from the queue.  No-one is noticing, it is transparent to the users.  Our servers go up and down
and no-one notices a thing.  And between proxy and Dally, the same thing as proxy but more specialised.  Macara I will talk about in a second.  Proxy is a little tool that came out of Twitter
and it is a connection pulling proxy for Reddis.  Let's actually look at a couple of examples of
resilience.  How much time do I have? Five.  We'll go through a couple of patterns.  This one is where we have application, talking to multiple services all through HP proxy and the
configuration is entirely condensed.  The app doesn't know where things are and shouldn't
because it is large.  Macara has been in production and us, and the two different, we should get together and merge them at some point.  But this is an example of configuration on the right and on the left the sort of features it does.  It balances with weights and so on.  The biggest thing about it works in a multi-threaded environment and we are in the world of multi-threaded Ruby. I really, really hate gems that are not thread safe.  Please don't write unsafe gems anymore. Replicate everything.  So, in this case we are receiving writes, we want to update our search engine, pushing into queue, backup work updates, pushes to replica, the replica is being read by the application.  You can lose the hosts without impact to the user.  The user is not going to notice anything.  You may slow down on the updates and not receive fresh data.  Another interesting pattern is while you have probably one or two load balancers, what happens if one goes down, you can probably devest $100,000 in a balancer or go with something which gives you, with enterprise account, 310 IPs it picks for you.  It one goes away, it will do whatever you tell it to do, remove it, switch it.  DNS works like magic.  We've had happen before.  It works
best with a short TTL.  We can figure load balances and pairs for partitioning.  They cannot ping us, maybe DDOS or something, we don't want to lose all the Rubies from the DNS pool, so that sort of takes care of that.  The other part I find really key, probably most of you are running rescue, sorry about that, but if you are using any kind of background processing you will probably be talking to register, into there.  If you have an application that is getting 100,000
KRPM, that is a lot.  What happens if it goes down? That is a failure.  So what we came up with is this failure-to-overflow pattern where we are keeping this, but unlike the previous configuration with round robins it will simply switch to it when the primary is completely overwhelmed.  That has saved us many, many times.  When it closed down, we just ignore it because who cares.  But what we do is things like standard drops and saves per second, basic business metrics, they are the key things you do want.  We receive 200 pages a week because hosts come up and down but application keeps running.  All of this requires really, really

obsessive monitoring and today monitoring is fantastic.  So many different options that probably a subject of an entirely different talk.  We actually run a dashboard that looks like this inside our office on a big TV, whenever anyone deploys anything they will receive something that goes into the red, for example, maybe an error spike or something else happened.  Just the reference, the tool is called Surconas, it is not free but it is fantastic for grafting it.  A cloud vendor is a partner, we get phenomenal support from Joint.  DevOps is just code.  Hire folks that write code.  Hire folks that have the rigor to approach the way we approach our DevOps and features.  The summary of all of this is here is how to reduce your stress: Insist on 100 per cent information. Deploy patterns whenever possible.  Pitch only on what is important to the business.  Monitor everything else obsessively.  Choose a cloud provider who will be your partner and talk to.  And infrastructure and software engineering.  Thank you very much.
