Hi everyone. Thank you so much for having me; I’m delighted to be here. I’m going to spend a few minutes talking to you about static typing. It’s a big subject and I can only sketch it here, so I’ve written up a more detailed version of this talk on my blog; if anything I say sounds interesting, you can find out more at that URL. It’ll be on my last slide too. In November
I was at RubyConf in San Diego, where Matz used his opening keynote to talk about what might come next for Ruby. In particular, he said that Ruby 3.0 may happen in the next 10 years and that
it might see new concurrency features, a just-in-time compiler (perhaps the LLVM JIT), and
static typing. That’s right, static typing. A lot of people were worried when they heard about that. And that’s understandable, because in the Ruby community we don’t get much direct exposure to static type systems, and they immediately make us think of undesirable languages like C and
Java. So I want to unpack Matz’s remarks a bit, and provide a bit of context and vocabulary around them, in the hope of encouraging a productive conversation about static typing in Ruby.
To cut to the chase, my interpretation is that Matz was proposing three specific ideas beyond just
“Ruby should have static typing”. Those ideas are: Structural typing, implicit typing and soft typing. And those ideas provide an important degree of subtlety beyond the words “static typing”
that would steer the language towards something very different from just “Ruby that feels like
Java”. So if you already know what all those words mean, I don’t have anything to tell you! But
if they’re new to you, I’d like to convince you that they represent interesting ideas that go beyond the features of the static type systems you may have used in the past. So maybe they’re
interesting to think about in the context of Ruby. I mean, mn, maybe they’re worthless! But let’s
be specific about what we’re considering first. As a way to get you into this subject, I’d like you

to think about how computers handle values. Imagine you could crack open your laptop or your smartphone, and zoom in to the microscopic level, and look deep down into its memory to see what was stored there. You’d see something like this: An undifferentiated soup of ones and zeroes. Binary data. What do those patterns of ones and zeroes mean? You can’t tell just by looking at them; they don’t have any inherent meaning, at least not an interesting one. So at the lowest level, every value inside a computer looks the same, just a pattern of ones and zeroes. But when we write code to manipulate a value, we usually have a specific interpretation in mind: A given pattern of ones and zeros being used by our program might represent an unsigned or signed integer quantity, or a floating-point quantity, or a letter of the alphabet, or the integer address of a memory location where some data structure is stored, and so on. Primitive operations inside a computer generally just assume that the values they’re given are supposed to be interpreted in a particular way. For example, at some level a computer will have different operations for “add
two integers” and “add two floating-point numbers”, because every pattern of bits has a
DIFFERENT MEANING depending on whether it’s being interpreted as an integer or a floating point number. If we add these two 32-bit patterns of ones and zeroes by interpreting them as unsigned integers, we get this result. If we add the same patterns of ones and zeroes by interpreting them as floating-point numbers, we get a different result. And of course if those bits represent something that’s not an absolute quantity, like a memory address or a timestamp, it might not make sense to add them at all.  This points to a fundamental problem in constructing computer programs: How do we, as programmers, avoid values being accidentally misinterpreted? If we store in memory some data that represents an unsigned integer, and then later on retrieve that data and use it as though it represents a floating point number or a pointer to a particular kind of data structure, the result of our program will be junk. The machine code that actually gets executed by CPUs doesn’t get involved in this issue at all. In this assembly code each register just contains an interpretation-agnostic pattern of bits - a word - and these instructions treat the register R1 first as an integer, then as the address of an integer, the address of a floating point number, an offset of some other address, and as the target address of a branch. This programming language is BCPL, a direct ancestor of C, and it’s ultimately a kind of
portable assembly language: Each value is a word of bits, and those bits get interpreted differently depending on which operator you use with them. In this factorial program, the variables I and N are populated with words representing integers, and they happen to be consistently used as arguments to integer multiplication and subtraction operators, which treat their contents as integers, so everything works fine. But in the same program we could just as well add I and N together while treating them as floating point numbers with this hash-plus floating point addition operator, to get a nonsense result.  Or we could multiply N by a floating point number with the hash-star operator, again giving nonsense because N’s contents represent an integer. Or we could even treat N as a pointer, interpreting its contents as an address to read from or write to, which would almost certainly be a disaster. Even these two floating point literals are being incorrectly multiplied with the integer multiplication operation to give a junk answer. Assembly and BCPL rely completely on the programmer to keep their own mental model of which values should be interpreted in which ways, and to make sure that every use of an operation is appropriate for the values it’s being applied to. Which brings us to type systems.
Type systems are a programming language technology designed to prevent this kind of error. The idea is that by incorporating a type system into a programming language, you can reduce (or
perhaps eliminate completely) the ability of the programmer to accidentally write code that misinterprets a value. Broadly speaking, a type system is a set of rules that expects some of the
“things in your program” to have pieces of “metadata” attached to them. I’m being intentionally vague here, because the exact meaning of “thing”, and the exact content of the metadata, depends
on the particular type system. But regardless of the details, those pieces of metadata are called types, and they contain information about the intended interpretation of the thing they’re attached
to. The rules of a type system describe how to check this metadata to prevent errors caused by misinterpretation. So the general idea is to take the implicit information about value
representation out of the mind of the programmer and put it into the computer as explicit metadata, make it part of the program somehow, so that it can be systematically checked for consistency with the operations used on those values. The design of type systems can get very
complicated, but there are three big obvious questions to answer: Firstly, where does the
metadata go? What are the “things” in your program that it gets attached to? Secondly, what does that metadata look like? What information does a “type” tell us about how to interpret a value?
And finally, where does that metadata come from? Who decides how values should be
interpreted? Let’s start with the first question. Where does the metadata go?  There are three main answers we see in the wild. The simplest possible answer is that the metadata goes nowhere: We don’t have any metadata at all. That’s the case for assembly language and BCPL.

Programming languages that do this are usually called UNTYPED languages. A certain kind of person calls them UNI-TYPED languages because they take the sardonic perspective that everything in the program has been annotated with the same single piece of metadata — and aha, you fool, I tricked you, it’s the empty piece of metadata. But that’s like saying that everyone at RubyConf Australia is riding a horse, it’s just that we all happen to be riding the null horse; it might be mathematically accurate, but it’s not enlightening, and, y’know, I don’t see any horses here, so why are you bringing horses into it in the first place? How did you get in here? Another popular answer is that the metadata goes on values. That means that our actual in-memory representation of values includes extra information about how they should be interpreted. And once we have a convention about how to read that information, the programming language’s operations can check it to make sure they’ve been given compatible arguments. This is how
Ruby works. Inside the C implementation of the official Ruby interpreter, MRI, there’s a data structure called RBasic, that contains a load of flags, and those flags contain information about whether a value is a string, a regular expression, an array, an instance of a user-defined class, and so on. Every Ruby object begins with an RBasic structure that contains these flags. MRI defines some macros and helper functions that are able to extract these flags from any value. So when the runtime wants to do some operation on a value, it can look at the type of the value and raise an exception if it’s not what it expected. Then the program can exit gracefully instead of giving a junk result. Programming languages that do this are called dynamically typed. “Dynamic”
because you don’t see the types just by LOOKING at the program; they appear on-the-fly in memory when the program is run. And the third popular answer to “where does the metadata go?” is: On pieces of source code. Rather than store metadata on values in memory when the program is running, we put it in the actual text of the program, as part of the syntax of expressions and statements. This is how a language like C works. In the source of a C program we write things like “int” and “long” and “void” to declare the types of variables and functions. The rules of the type system allow all the syntactic occurrences of variables and functions to be checked to make sure the metadata from their declaration matches the way they’re being used. If you get this wrong, your program won’t even compile. These are called statically typed programming languages. You don’t need to run a statically typed program to check its metadata, because it’s written right there in the source code. So those are the three main ways of answering the question: There is no metadata; or there is metadata on values; or there’s metadata on pieces of source code. And there’s a spectrum of increasing safety here: If your program interprets a value wrongly, a static type system can tell you about your mistake at compile time before you ever run the program; a dynamic type system can tell you at the exact moment of misinterpretation, during the execution of the program; and an untyped language will never tell you, it’ll just silently produce garbage. All else being equal, safer is better. (All else isn’t equal, of course, and you can read my blog post for more on that.) The second big question was: What does the metadata look like? This question is independent of whether the types are static or dynamic; it’s about deciding what kind of information we want to record about how to interpret values, regardless of where we record it. Most type systems you’ve seen are probably nominal type systems. This means that types are just names, like in the C example, which uses names like “int” and “long” and “void” to identify types. This is by far the most common kind of metadata; it’s what you’re used to seeing in C and Java and virtually all mainstream programming languages. But there’s an alternative answer, which is that instead of types being simple names,
they can be structures. And implicit in that idea is that you can compare types by comparing their structures and deciding whether they’re compatible, rather than just checking whether or not two names are the same. That arrangement is called a structural type system. Here’s an example of what I mean, taken from the Go language. Go lets you declare an interface called Shape, and say
a Shape is anything with a method called “Area” that returns a float. This is the “structure” I’m talking about: Although the Shape interface does have a name for convenience, it’s not the name that’s important, it’s the structure that it refers to. (The “structure” in question here is just a list of method signatures.) Once you have this interface, you can invent types that conform to it. Here
are types called Rectangle and Circle that both have implementations of an Area method. Neither of them explicitly mention the Shape interface, but they have a matching structure. And that means we can use Rectangle and Circle values wherever a Shape is expected. We don’t have to declare explicit inheritance, like in a nominally typed language; we can just use a Rectangle or a Circle wherever a Shape is expected, because the structure of Rectangle and Circle is compatible with the structure of Shape. (This might remind you a bit of duck typing.) Last big question: Where does the metadata come from? This one only really applies in the context of statically typed languages, where information has to get attached to pieces of source code somehow. The most obvious answer is that the programmer provides this information by explicitly annotating
the source code; after all, the whole point is to capture knowledge from their brain. Going back

to the C factorial example: To get this code to compile, we have to actually peck out the strings “long” and “int” at every variable and function declaration while we write the program — or, if we’re lucky, get our editor or IDE to do it for us. This is called manifest typing, and the majority of mainstream statically typed languages work like this; they expect the programmer to do all (or most) of the busywork of annotating the program. The alternative is to let the computer — usually the compiler — add the type annotations itself. Every time the programmer uses a particular operation (e.g. integer addition), they’re leaking a piece of information about what
kind of values (e.g. integers) they’re expecting to see at that particular point in their code. For some languages it’s possible to use a type inference algorithm to move this partial information
around, join it together, fill in the gaps, and eventually reconstruct complete type annotations for every expression in the program, sometimes without needing the programmer to label anything at
all. This is a factorial program written in Haskell’s interactive console. It looks more like a dynamically typed program than a statically typed one; it’s not very different from what we’d
write in Ruby or JavaScript. But it’s just as statically typed as the C version, because Haskell can infer all the type annotations just by looking at how we’ve used the built-in operations and
literals, so the type of everything is completely known before the code ever gets executed. If we ask Haskell what the type of factorial is, it tells us it’s a function which takes any numeric argument that supports equality comparisons. Using type inference to reconstruct source code
metadata like this instead of requiring explicit annotations is sometimes called implicit typing. The point of this talk was to unpack what Matz said, but it’s almost over and I haven’t even
mentioned what he said yet. Let’s have a quick look at his slides and try to work out what he
thinks Ruby 3.0 will look like. He does help us out by showing some speculative example code on this slide; he sets `a` equal to 1 and comments that the type of `a` is `Integer`. Here’s another
example he gave. He defines a method called `foo` that “requires x to have a method called
to_int”, and then he shows two uses of `foo`, one of which is okay because the object passed in has a #to_int method, and the other of which is not okay. On a third slide he says “a type is represented by a set of methods”, and notes that you could use a class name as shorthand for that. Taken together, I think those slides make it pretty clear where Matz is going. Firstly, it’s obvious that he’s talking about Ruby’s type system being static, not (just) dynamic: Metadata on pieces of source code, not (just) on run-time values. He just came out and said that. But he’s also clearly talking about using structural types, not (just) nominal ones: Those slides say that the metadata should talk about what messages a value responds to, not simply what class it’s an instance of. That’s important because it would preserve some of the feel of duck typing; the type of a method argument can specify that it needs to respond to certain messages, and then it will accept any object whose type says it responds to them. And the code he showed didn’t have any annotations, which suggests Matz wants an implicit type system that uses type inference to generate as much of the static metadata as possible, instead of making programmers type it in explicitly. BIG Q: How would this implicit, structural, static type system actually work? Making all that work for Ruby is a difficult job. For example, integrating structural typing with open classes is hard, because if new methods can be added to the String class at any time while the program is
running, how do you know what set of method signatures to put in the String type? Matz made a couple of references to a paper from 1991 called “Soft Typing”, which describes a “best of both
worlds” type system that works by transforming a program at compile time: A type inference
algorithm tries to reconstruct static types for the whole program, and in the parts where it fails, it forces those parts to become statically typed by inserting dynamic checks. This turns a situation
like “these two values may or may not be numbers” into “you can safely proceed on the
assumption that these two values are numbers, because an exception will be thrown at run time if they’re not”. This sounds appealing, but reading the paper made me sceptical of how its ideas could be applied to Ruby, for a few reasons: Firstly, the soft type system is defined for an “idealised functional language”, namely a very simple toy language that doesn’t exhibit any of Ruby’s interesting features; it doesn’t have objects, for example. Secondly, while it does have a clever type inference algorithm, the soft type system doesn’t look structural to me, so it doesn’t seem like it helps with that particular design goal. Finally, it doesn’t actually provide any safety. The goal of soft typing is to make dynamic programs faster by eliminating run time checks; instead of having a fully dynamic program, parts of it can be made statically typed, so that only the remaining parts need to be checked when it runs. But Matz said that we don’t need static typing for speed; he mentioned how successful the V8 and LuaJIT projects have been at making dynamic languages run fast. That leaves me confused about what benefits soft typing would bring to Ruby. So, what’s the next step? What should the Ruby community do to start making progress on this? We have a big challenge ahead of us: Can we bring the safety, performance and documentation benefits of static typing to Ruby without compromising its flexibility, its friendliness, its humanity? My technical opinion is that we can do it, and my non-technical

opinion is that we should do it if we want Ruby to remain competitive and relevant in the next decade. Almost all new languages have some kind of static type system; Rust is a good example of a recent language that’s not only statically typed, but which also pushes the boundaries of mainstream type systems by including an ownership system for statically tracking which part of the program is responsible for each allocated region of memory. Facebook released Flow just
over a year ago, which is turning out to be a very capable static type checker for JavaScript. Even
Perl 6 has optional support for static types, for god’s sake. There’s no point in Ruby being fashionable just for the sake of fashion, but these new languages are servicing a genuine and growing demand for computers to understand our programs better so they can help us to get them right and make them fast. Safety and software quality is the most important reason why we
should be considering this. Imagine being able to catch a broken view helper the first time your
Rails application boots on the CI server, instead of when someone hits an obscure page on the admin interface two weeks after it’s been deployed to production. Writing tests is great, but a static type system is like a built-in test suite that catches all the ridiculous, pointless problems
you waste time on every day, so that you can focus on the interesting ones.  We have no reason to fear types. Ruby already has them; they already get checked. The only question is when they
should be checked, and the earlier we can check them, the more likely it is that we’ll be able to
catch silly mistakes before they make it into production. And that’s a good thing, as long as we can come up with a static type system that understands the kind of programs we want to be able to write in Ruby. These are hard problems, though. Designing a static type system that captures
enough safety to be useful, while preserving enough flexibility to still feel like Ruby, is a serious
formal challenge. The “soft typing” paper doesn’t provide the answers for a language like Ruby. I
don’t know of anything we can take off the shelf here. Most of the PIECES exist, but to fit them all together with Ruby requires attention to mathematical detail as well as a sensitivity to Ruby’s aesthetic qualities. It’s going to take serious research, not brute-force hacking; the answer will probably look more like a PhD thesis than a pull request. But there’s no reason to be defeatist. There are plenty of smart and motivated people in the Ruby community; there are plenty of interested companies that have the money to pay research staff or fund a PhD studentship; there is plenty of desire to keep Ruby moving forward and make it a viable and relevant language for the next decade. I hope what I’ve said today can help to get those conversations started. Thanks very much.
